{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('text_emotion.csv')\n",
    "df.head()\n",
    "df = df[pd.notnull(df['content'])]\n",
    "col=['sentiment','content']\n",
    "df=df[col]\n",
    "df.columns\n",
    "df.columns=['sentiment','content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sentiment                                            content\n",
      "0       empty  i know i was listenin to bad habit earlier and...\n",
      "1     sadness  Layin n bed with a headache ughhhh waitin on y...\n",
      "2     sadness                     Funeral ceremony gloomy friday\n",
      "3  enthusiasm                wants to hang out with friends SOON\n",
      "4     neutral  We want to trade with someone who has Houston ...\n",
      "5       worry  Re pinging why didn t you go to prom BC my bf ...\n",
      "6     sadness  I should be sleep but im not thinking about an...\n",
      "7       worry                                       Hmmm is down\n",
      "8     sadness                        Charlene my love I miss you\n",
      "9     sadness                     I m sorry at least it s Friday\n"
     ]
    }
   ],
   "source": [
    "def clean_tweet(tweet):\n",
    "    '''\n",
    "    Utility function to clean the text in a tweet by removing \n",
    "    links and special characters using regex.\n",
    "    '''\n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "\n",
    "df['content'] = np.array([ clean_tweet(tweet) for tweet in df['content'] ])\n",
    "print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_fatures = 2000\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "tokenizer.fit_on_texts(df['content'].values)\n",
    "X = tokenizer.texts_to_sequences(df['content'].values)\n",
    "X = pad_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 36, 128)           256000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_2 (Spatial (None, 36, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 196)               254800    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 13)                2561      \n",
      "=================================================================\n",
      "Total params: 513,361\n",
      "Trainable params: 513,361\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(13,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26800, 36) (26800, 13)\n",
      "(13200, 36) (13200, 13)\n"
     ]
    }
   ],
   "source": [
    "Y = pd.get_dummies(df['sentiment']).values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      " - 106s - loss: 2.0457 - acc: 0.2913\n",
      "Epoch 2/7\n",
      " - 109s - loss: 1.8854 - acc: 0.3610\n",
      "Epoch 3/7\n",
      " - 120s - loss: 1.8240 - acc: 0.3806\n",
      "Epoch 4/7\n",
      " - 104s - loss: 1.7780 - acc: 0.3981\n",
      "Epoch 5/7\n",
      " - 99s - loss: 1.7406 - acc: 0.4088\n",
      "Epoch 6/7\n",
      " - 100s - loss: 1.6967 - acc: 0.4183\n",
      "Epoch 7/7\n",
      " - 98s - loss: 1.6640 - acc: 0.4323\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17a026a5780>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model.fit(X_train, Y_train, epochs = 7, batch_size=batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 1.94\n",
      "acc: 0.35\n"
     ]
    }
   ],
   "source": [
    "validation_size = 1500\n",
    "\n",
    "X_validate = X_test[-validation_size:]\n",
    "Y_validate = Y_test[-validation_size:]\n",
    "X_test = X_test[:-validation_size]\n",
    "Y_test = Y_test[:-validation_size]\n",
    "score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
